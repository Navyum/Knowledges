<!doctype html>
<html lang="zh-CN" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="keywords" content="自我提升,效率提升,职场经验,学习笔记" />
    <meta name="360-site-verification" content="deb32fab1ab57ddf02d8c5b4cc37025c" />
    <meta name="msvalidate.01" content="034FE00C2DAE4BC3B10D36ABBE467614" />
    <style>
      :root {
        --vp-c-bg: #fff;
      }

      [data-theme="dark"] {
        --vp-c-bg: #4089ab;
      }

      html,
      body {
        background: var(--vp-c-bg);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode = window.matchMedia && window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.documentElement.setAttribute("data-theme", "dark");
      }
    </script>
    <!--  #252232 --> 
    <meta property="og:url" content="https://myblog.camscanner.top/%E5%A4%A7%E6%A8%A1%E5%9E%8BLLM/00.%E6%A6%82%E5%BF%B5/02.GPT.html"><meta property="og:site_name" content="Navyum's Blog"><meta property="og:title" content="02.GPT"><meta property="og:description" content="GPT： Generative Pre-Trained Transformer即生成式预训练转换器，其架构基于原始的 transformer 的解码器 GPT主要训练阶段： 无监督预训练PT（Unsupervised Pre-training）： 定义：在未标记的文本上预训练 GPT，从而利用丰富的文本语料库。该阶段又叫做生成式预训练。 任务：训练模型..."><meta property="og:type" content="article"><meta property="og:image" content="https://raw.staticdn.net/Navyum/imgbed/pic/IMG/026b670dec056d8526150d6774badceb.png"><meta property="og:locale" content="zh-CN"><meta property="og:updated_time" content="2025-06-17T07:42:14.000Z"><meta property="article:author" content="Navyum"><meta property="article:tag" content="LLM"><meta property="article:tag" content="GPT"><meta property="article:published_time" content="2024-11-12T16:47:40.000Z"><meta property="article:modified_time" content="2025-06-17T07:42:14.000Z"><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"02.GPT","image":["https://raw.staticdn.net/Navyum/imgbed/pic/IMG/026b670dec056d8526150d6774badceb.png","https://raw.staticdn.net/Navyum/imgbed/pic/IMG/5053a72bfd83e3cd7408677d1ee5cbab.png","https://raw.staticdn.net/Navyum/imgbed/pic/IMG/7f3ff204a9477f831a32fded2f10d10c.png","https://raw.staticdn.net/Navyum/imgbed/pic/IMG/962dd7a9f23f11285c539e784d5f4d98.png"],"datePublished":"2024-11-12T16:47:40.000Z","dateModified":"2025-06-17T07:42:14.000Z","author":[{"@type":"Person","name":"Navyum"}]}</script><link rel="alternate" type="application/atom+xml" href="https://myblog.camscanner.top/atom.xml" title="Navyum's Blog Atom Feed"><link rel="alternate" type="application/json" href="https://myblog.camscanner.top/feed.json" title="Navyum's Blog JSON Feed"><link rel="alternate" type="application/rss+xml" href="https://myblog.camscanner.top/rss.xml" title="Navyum's Blog RSS Feed"><link rel="icon" href="/favicon.ico"><title>02.GPT | Navyum's Blog</title><meta name="description" content="GPT： Generative Pre-Trained Transformer即生成式预训练转换器，其架构基于原始的 transformer 的解码器 GPT主要训练阶段： 无监督预训练PT（Unsupervised Pre-training）： 定义：在未标记的文本上预训练 GPT，从而利用丰富的文本语料库。该阶段又叫做生成式预训练。 任务：训练模型...">
    <link rel="stylesheet" href="/assets/css/styles.08b9eca3.css">
    <link rel="preload" href="/assets/js/runtime~app.4343b326.js" as="script"><link rel="preload" href="/assets/css/styles.08b9eca3.css" as="style"><link rel="preload" href="/assets/js/1181.88d0b359.js" as="script"><link rel="preload" href="/assets/js/app.0aa5305f.js" as="script">
    
    <!-- 统计代码区域-->
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="vp-skip-link sr-only">跳至主要內容</a><!--]--><div class="theme-container external-link-icon has-toc" vp-container><!--[--><header id="navbar" class="vp-navbar" vp-navbar><div class="vp-navbar-start"><button type="button" class="vp-toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!--[--><a class="route-link vp-brand" href="/" aria-label="带我回家"><img class="vp-nav-logo" src="/logo.svg" alt><!----><span class="vp-site-name hide-in-pad">Navyum&#39;s Blog</span></a><!--]--></div><div class="vp-navbar-center"><!--[--><nav class="vp-nav-links"><div class="vp-nav-item hide-in-mobile"><a class="route-link auto-link" href="/" aria-label="主页" iconsizing="height"><!--[--><iconify-icon class="vp-icon" icon="meteor-icons:home" height="1em" sizing="height"></iconify-icon><!--]-->主页<!----></a></div><div class="vp-nav-item hide-in-mobile"><a class="route-link auto-link" href="/blog.html" aria-label="博客" iconsizing="height"><!--[--><iconify-icon class="vp-icon" icon="fa6-solid:blog" height="1em" sizing="height"></iconify-icon><!--]-->博客<!----></a></div><div class="vp-nav-item hide-in-mobile"><div class="vp-dropdown-wrapper"><button type="button" class="vp-dropdown-title" aria-label="思考"><!--[--><iconify-icon class="vp-icon" icon="icon-park-outline:thinking-problem" height="1em" sizing="height"></iconify-icon>思考<!--]--><span class="arrow"></span><ul class="vp-dropdown"><li class="vp-dropdown-item"><a class="route-link auto-link" href="/%E9%9D%A2%E8%AF%95%E7%BB%8F%E9%AA%8C/" aria-label="面试经验" iconsizing="both"><!--[--><iconify-icon class="vp-icon" icon="fluent:people-chat-24-regular" width="1em" height="1em" sizing="both"></iconify-icon><!--]-->面试经验<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/%E4%B8%AA%E4%BA%BA%E6%80%9D%E8%80%83/" aria-label="个人思考" iconsizing="both"><!--[--><iconify-icon class="vp-icon" icon="icon-park-outline:thinking-problem" width="1em" height="1em" sizing="both"></iconify-icon><!--]-->个人思考<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/%E6%88%91%E7%9A%84%E4%B8%96%E7%95%8C/" aria-label="我的世界" iconsizing="both"><!--[--><iconify-icon class="vp-icon" icon="streamline-plump:world-remix" width="1em" height="1em" sizing="both"></iconify-icon><!--]-->我的世界<!----></a></li></ul></button></div></div><div class="vp-nav-item hide-in-mobile"><a class="auto-link external-link" href="https://www.notion.so/navyum/1c42fcd1fefa4e948d8514761b2ab8c7?v=0ca5dc6ee29e4c2787dbd0f1055b4ed0" aria-label="读书" rel="noopener noreferrer" target="_blank" iconsizing="height"><!--[--><iconify-icon class="vp-icon" icon="fa6-brands:readme" height="1em" sizing="height"></iconify-icon><!--]-->读书<!----></a></div><div class="vp-nav-item hide-in-mobile"><a class="route-link auto-link" href="/intro.html" aria-label="联系" iconsizing="height"><!--[--><iconify-icon class="vp-icon" icon="streamline-plump:contact-phonebook-solid" height="1em" sizing="height"></iconify-icon><!--]-->联系<!----></a></div><div class="vp-nav-item hide-in-mobile"><a class="route-link auto-link" href="/map.html" aria-label="导航" iconsizing="height"><!--[--><iconify-icon class="vp-icon" icon="streamline-plump:world-remix" height="1em" sizing="height"></iconify-icon><!--]-->导航<!----></a></div></nav><!--]--></div><div class="vp-navbar-end"><!--[--><div class="vp-nav-item vp-action"><a class="vp-action-link" href="https://github.com/navyum" target="_blank" rel="noopener noreferrer" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" class="icon github-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="github icon" name="github" style="width:1.25rem;height:1.25rem;vertical-align:middle;"><path d="M511.957 21.333C241.024 21.333 21.333 240.981 21.333 512c0 216.832 140.544 400.725 335.574 465.664 24.49 4.395 32.256-10.07 32.256-23.083 0-11.69.256-44.245 0-85.205-136.448 29.61-164.736-64.64-164.736-64.64-22.315-56.704-54.4-71.765-54.4-71.765-44.587-30.464 3.285-29.824 3.285-29.824 49.195 3.413 75.179 50.517 75.179 50.517 43.776 75.008 114.816 53.333 142.762 40.79 4.523-31.66 17.152-53.377 31.19-65.537-108.971-12.458-223.488-54.485-223.488-242.602 0-53.547 19.114-97.323 50.517-131.67-5.035-12.33-21.93-62.293 4.779-129.834 0 0 41.258-13.184 134.912 50.346a469.803 469.803 0 0 1 122.88-16.554c41.642.213 83.626 5.632 122.88 16.554 93.653-63.488 134.784-50.346 134.784-50.346 26.752 67.541 9.898 117.504 4.864 129.834 31.402 34.347 50.474 78.123 50.474 131.67 0 188.586-114.73 230.016-224.042 242.09 17.578 15.232 33.578 44.672 33.578 90.454v135.85c0 13.142 7.936 27.606 32.854 22.87C862.25 912.597 1002.667 728.747 1002.667 512c0-271.019-219.648-490.667-490.71-490.667z"></path></svg></a></div><div class="vp-nav-item hide-in-mobile"><button type="button" class="vp-outlook-button" tabindex="-1" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" class="icon outlook-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="outlook icon" name="outlook"><path d="M224 800c0 9.6 3.2 44.8 6.4 54.4 6.4 48-48 76.8-48 76.8s80 41.6 147.2 0 134.4-134.4 38.4-195.2c-22.4-12.8-41.6-19.2-57.6-19.2C259.2 716.8 227.2 761.6 224 800zM560 675.2l-32 51.2c-51.2 51.2-83.2 32-83.2 32 25.6 67.2 0 112-12.8 128 25.6 6.4 51.2 9.6 80 9.6 54.4 0 102.4-9.6 150.4-32l0 0c3.2 0 3.2-3.2 3.2-3.2 22.4-16 12.8-35.2 6.4-44.8-9.6-12.8-12.8-25.6-12.8-41.6 0-54.4 60.8-99.2 137.6-99.2 6.4 0 12.8 0 22.4 0 12.8 0 38.4 9.6 48-25.6 0-3.2 0-3.2 3.2-6.4 0-3.2 3.2-6.4 3.2-6.4 6.4-16 6.4-16 6.4-19.2 9.6-35.2 16-73.6 16-115.2 0-105.6-41.6-198.4-108.8-268.8C704 396.8 560 675.2 560 675.2zM224 419.2c0-28.8 22.4-51.2 51.2-51.2 28.8 0 51.2 22.4 51.2 51.2 0 28.8-22.4 51.2-51.2 51.2C246.4 470.4 224 448 224 419.2zM320 284.8c0-22.4 19.2-41.6 41.6-41.6 22.4 0 41.6 19.2 41.6 41.6 0 22.4-19.2 41.6-41.6 41.6C339.2 326.4 320 307.2 320 284.8zM457.6 208c0-12.8 12.8-25.6 25.6-25.6 12.8 0 25.6 12.8 25.6 25.6 0 12.8-12.8 25.6-25.6 25.6C470.4 233.6 457.6 220.8 457.6 208zM128 505.6C128 592 153.6 672 201.6 736c28.8-60.8 112-60.8 124.8-60.8-16-51.2 16-99.2 16-99.2l316.8-422.4c-48-19.2-99.2-32-150.4-32C297.6 118.4 128 291.2 128 505.6zM764.8 86.4c-22.4 19.2-390.4 518.4-390.4 518.4-22.4 28.8-12.8 76.8 22.4 99.2l9.6 6.4c35.2 22.4 80 12.8 99.2-25.6 0 0 6.4-12.8 9.6-19.2 54.4-105.6 275.2-524.8 288-553.6 6.4-19.2-3.2-32-19.2-32C777.6 76.8 771.2 80 764.8 86.4z"></path></svg><div class="vp-outlook-dropdown"><!----></div></button></div><!--[--><div id="docsearch-container" style="display:none;"></div><div><button type="button" class="DocSearch DocSearch-Button" aria-label="搜索文档"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">搜索文档</span></span><span class="DocSearch-Button-Keys"><kbd class="DocSearch-Button-Key"><svg width="15" height="15" class="DocSearch-Control-Key-Icon"><path d="M4.505 4.496h2M5.505 5.496v5M8.216 4.496l.055 5.993M10 7.5c.333.333.5.667.5 1v2M12.326 4.5v5.996M8.384 4.496c1.674 0 2.116 0 2.116 1.5s-.442 1.5-2.116 1.5M3.205 9.303c-.09.448-.277 1.21-1.241 1.203C1 10.5.5 9.513.5 8V7c0-1.57.5-2.5 1.464-2.494.964.006 1.134.598 1.24 1.342M12.553 10.5h1.953" stroke-width="1.2" stroke="currentColor" fill="none" stroke-linecap="square"></path></svg></kbd><kbd class="DocSearch-Button-Key">K</kbd></span></button></div><!--]--><!--]--><button type="button" class="vp-toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span><span class="vp-top"></span><span class="vp-middle"></span><span class="vp-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow start"></span></div><aside id="sidebar" class="vp-sidebar" vp-sidebar><!----><ul class="vp-sidebar-links"><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable active" type="button"><iconify-icon class="vp-icon" icon="ri:ai-generate-2" width="1em" height="1em" sizing="both"></iconify-icon><span class="vp-sidebar-title">大模型LLM</span><span class="vp-arrow down"></span></button><ul class="vp-sidebar-links"><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable active" type="button"><!----><span class="vp-sidebar-title">00.概念</span><span class="vp-arrow down"></span></button><ul class="vp-sidebar-links"><li><a class="route-link auto-link vp-sidebar-link" href="/%E5%A4%A7%E6%A8%A1%E5%9E%8BLLM/00.%E6%A6%82%E5%BF%B5/01.Transformer.html" aria-label="01.Transformer" iconsizing="both"><!---->01.Transformer<!----></a></li><li><a class="route-link route-link-active auto-link vp-sidebar-link active" href="/%E5%A4%A7%E6%A8%A1%E5%9E%8BLLM/00.%E6%A6%82%E5%BF%B5/02.GPT.html" aria-label="02.GPT" iconsizing="both"><!---->02.GPT<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/%E5%A4%A7%E6%A8%A1%E5%9E%8BLLM/00.%E6%A6%82%E5%BF%B5/03.Prompt.html" aria-label="03.Prompt" iconsizing="both"><!---->03.Prompt<!----></a></li><li><a class="route-link auto-link vp-sidebar-link" href="/%E5%A4%A7%E6%A8%A1%E5%9E%8BLLM/00.%E6%A6%82%E5%BF%B5/04.Models.html" aria-label="04.Models" iconsizing="both"><!---->04.Models<!----></a></li></ul></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">01.框架</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">02. RAG</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">04.应用</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">README</span><span class="vp-arrow end"></span></button><!----></section></li></ul></section></li></ul><!----></aside><!--[--><main id="main-content" class="vp-page"><!--[--><!----><!----><nav class="vp-breadcrumb disable"></nav><div class="vp-page-title"><h1><!---->02.GPT</h1><div class="page-info"><span class="page-category-info" aria-label="分类🌈" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon category-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="category icon" name="category"><path d="M148.41 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H148.41c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.311-40.31zM147.556 553.478H429.73c22.263 0 40.311 18.048 40.311 40.31v282.176c0 22.263-18.048 40.312-40.31 40.312H147.555c-22.263 0-40.311-18.049-40.311-40.312V593.79c0-22.263 18.048-40.311 40.31-40.311zM593.927 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H593.927c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.31-40.31zM730.22 920.502H623.926c-40.925 0-74.22-33.388-74.22-74.425V623.992c0-41.038 33.387-74.424 74.425-74.424h222.085c41.038 0 74.424 33.226 74.424 74.067v114.233c0 10.244-8.304 18.548-18.547 18.548s-18.548-8.304-18.548-18.548V623.635c0-20.388-16.746-36.974-37.33-36.974H624.13c-20.585 0-37.331 16.747-37.331 37.33v222.086c0 20.585 16.654 37.331 37.126 37.331H730.22c10.243 0 18.547 8.304 18.547 18.547 0 10.244-8.304 18.547-18.547 18.547z"></path></svg><!--[--><span class="page-category-item color6 clickable" role="navigation">LLM</span><span class="page-category-item color5 clickable" role="navigation">AI</span><!--]--><meta property="articleSection" content="LLM,AI"></span><span class="page-tag-info" aria-label="标签🏷" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon tag-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="tag icon" name="tag"><path d="M939.902 458.563L910.17 144.567c-1.507-16.272-14.465-29.13-30.737-30.737L565.438 84.098h-.402c-3.215 0-5.726 1.005-7.634 2.913l-470.39 470.39a10.004 10.004 0 000 14.164l365.423 365.424c1.909 1.908 4.42 2.913 7.132 2.913s5.223-1.005 7.132-2.913l470.39-470.39c2.01-2.11 3.014-5.023 2.813-8.036zm-240.067-72.121c-35.458 0-64.286-28.828-64.286-64.286s28.828-64.285 64.286-64.285 64.286 28.828 64.286 64.285-28.829 64.286-64.286 64.286z"></path></svg><!--[--><span class="page-tag-item color6 clickable" role="navigation">LLM</span><span class="page-tag-item color5 clickable" role="navigation">GPT</span><!--]--><meta property="keywords" content="LLM,GPT"></span><span class="page-word-info" aria-label="字数🔠" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon word-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="word icon" name="word"><path d="M518.217 432.64V73.143A73.143 73.143 0 01603.43 1.097a512 512 0 01419.474 419.474 73.143 73.143 0 01-72.046 85.212H591.36a73.143 73.143 0 01-73.143-73.143z"></path><path d="M493.714 566.857h340.297a73.143 73.143 0 0173.143 85.577A457.143 457.143 0 11371.566 117.76a73.143 73.143 0 0185.577 73.143v339.383a36.571 36.571 0 0036.571 36.571z"></path></svg><span>约 1019 字</span><meta property="wordCount" content="1019"></span><span class="page-reading-time-info" aria-label="阅读时间⌛" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="timer icon" name="timer"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>大约 3 分钟</span><meta property="timeRequired" content="PT3M"></span><span class="page-pageview-info" aria-label="访问量🔢" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon eye-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="eye icon" name="eye"><path d="M992 512.096c0-5.76-.992-10.592-1.28-11.136-.192-2.88-1.152-8.064-2.08-10.816-.256-.672-.544-1.376-.832-2.08-.48-1.568-1.024-3.104-1.6-4.32C897.664 290.112 707.104 160 512 160c-195.072 0-385.632 130.016-473.76 322.592-1.056 2.112-1.792 4.096-2.272 5.856a55.512 55.512 0 00-.64 1.6c-1.76 5.088-1.792 8.64-1.632 7.744-.832 3.744-1.568 11.168-1.568 11.168-.224 2.272-.224 4.032.032 6.304 0 0 .736 6.464 1.088 7.808.128 1.824.576 4.512 1.12 6.976h-.032c.448 2.08 1.12 4.096 1.984 6.08.48 1.536.992 2.976 1.472 4.032C126.432 733.856 316.992 864 512 864c195.136 0 385.696-130.048 473.216-321.696 1.376-2.496 2.24-4.832 2.848-6.912.256-.608.48-1.184.672-1.728 1.536-4.48 1.856-8.32 1.728-8.32l-.032.032c.608-3.104 1.568-7.744 1.568-13.28zM512 672c-88.224 0-160-71.776-160-160s71.776-160 160-160 160 71.776 160 160-71.776 160-160 160z"></path></svg><span id="ArtalkPV" class="vp-pageview waline-pageview-count" data-path="/%E5%A4%A7%E6%A8%A1%E5%9E%8BLLM/00.%E6%A6%82%E5%BF%B5/02.GPT.html" data-page-key="/%E5%A4%A7%E6%A8%A1%E5%9E%8BLLM/00.%E6%A6%82%E5%BF%B5/02.GPT.html">...</span></span></div><hr></div><!----><!----><div class="theme-hope-content" vp-content><h2 id="gpt" tabindex="-1"><a class="header-anchor" href="#gpt"><span>GPT：</span></a></h2><ul><li>Generative Pre-Trained Transformer即生成式预训练转换器，其架构基于原始的 transformer 的<strong>解码器</strong></li></ul><h2 id="gpt主要训练阶段" tabindex="-1"><a class="header-anchor" href="#gpt主要训练阶段"><span>GPT主要训练阶段：</span></a></h2><h3 id="无监督预训练pt-unsupervised-pre-training" tabindex="-1"><a class="header-anchor" href="#无监督预训练pt-unsupervised-pre-training"><span>无监督预训练PT（Unsupervised Pre-training）：</span></a></h3><p>定义：在未标记的文本上预训练 GPT，从而利用丰富的文本语料库。该阶段又叫做生成式预训练。 任务：训练模型以了解语言的结构并捕获文本数据集中存在的统计模式。它不是针对特定的语言任务，而是<span style="color:rgb(255, 41, 65);">提高模型对语言本身的理解</span>。 具体：无监督预训练将一系列标记提供给模型（Transformer 解码器的变体）以预测下一个标记的概率。它在下图中显示为 “Text Prediction” （其中“Task Classifier” 用于监督微调SFT阶段） <img src="https://raw.staticdn.net/Navyum/imgbed/pic/IMG/026b670dec056d8526150d6774badceb.png" alt="Img" loading="lazy"></p><h3 id="监督式微调sft-supervised-fine-tuning" tabindex="-1"><a class="header-anchor" href="#监督式微调sft-supervised-fine-tuning"><span>监督式微调SFT（Supervised Fine-tuning）：</span></a></h3><p>使用标记数据为每个特定任务微调预训练模型，又叫做判别性微调。</p><h4 id="简单分类任务" tabindex="-1"><a class="header-anchor" href="#简单分类任务"><span>简单分类任务：</span></a></h4><p>对于简单的分类任务，每个输入都由一系列标记和一个标签组成。它们将输入传递给预先训练的模型以获得最终的激活值，这些激活值被馈送到额外的线性 （+softmax） 输出层中</p><h4 id="文本蕴涵任务" tabindex="-1"><a class="header-anchor" href="#文本蕴涵任务"><span>文本蕴涵任务：</span></a></h4><p>将一对文本之间的关系进行分类。在给定前提（文本）的情况下推断假设另一个（文本）是正确的，那么这种关系就被归类为蕴涵。如果两者之间存在不一致，则关系被归类为矛盾。如果两者都不是真的，则关系被归类为中性。</p><h4 id="相似度任务" tabindex="-1"><a class="header-anchor" href="#相似度任务"><span>相似度任务：</span></a></h4><p>比较的两个句子之间没有方向关系（余弦法）</p><h4 id="问答和常识推理任务" tabindex="-1"><a class="header-anchor" href="#问答和常识推理任务"><span>问答和常识推理任务</span></a></h4><p>这些任务提供了一个上下文文档<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span></span></span></span> 、一个问题<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">q</span></span></span></span>和一组可能的答案<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>a</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">{a_k}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span>。对于每个可能的答案，都有一个输入序列。</p><h2 id="gpt-2" tabindex="-1"><a class="header-anchor" href="#gpt-2"><span>GPT-2：</span></a></h2><ul><li><p>GPT-2 是 GPT-1 的直接扩展，具有更多参数并在更多数据上进行训练（1.5B）</p><table><thead><tr><th style="text-align:left;">差异</th><th style="text-align:left;">GPT-1</th><th style="text-align:left;">GPT-2</th><th style="text-align:left;">GPT-3</th></tr></thead><tbody><tr><td style="text-align:left;">解码器个数</td><td style="text-align:left;">12</td><td style="text-align:left;">48</td><td style="text-align:left;">96</td></tr><tr><td style="text-align:left;">参数大小</td><td style="text-align:left;">0.15B</td><td style="text-align:left;">1.5B</td><td style="text-align:left;">17.5B</td></tr><tr><td style="text-align:left;">向量维度</td><td style="text-align:left;">512</td><td style="text-align:left;">1600</td><td style="text-align:left;">12288</td></tr></tbody></table></li><li><p>在GPT-1的实验中，在没有监督微调的情况下，语言模型已经包含执行特定任务所需的信息。即所有这些知识都存储在网络参数（权重和偏差）中。</p></li><li><p>因此，更多的参数应该会增加语言模型的容量，<strong>微调只是为特定任务的模型添加了最后的润色</strong>，因此使 GPT出色的主要因素是预训练</p></li></ul><h2 id="gpt-3" tabindex="-1"><a class="header-anchor" href="#gpt-3"><span>GPT-3：</span></a></h2><p>GPT-3 显示了纯粹通过文本交互处理任务的改进能力。这些任务包括零样本（Zero-shot）、单次（one-shot）和少量样本（few-shot）示例的学习，并且必须在没有额外训练的情况下（不使用微调）执行任务。</p><h3 id="元学习" tabindex="-1"><a class="header-anchor" href="#元学习"><span>元学习：</span></a></h3><ul><li>定义：能够学习根据过去的经验进行组合预测；学会了使用之前获得的技能来理解和实现所需的新任务</li></ul><h4 id="上下文学习-情境学习" tabindex="-1"><a class="header-anchor" href="#上下文学习-情境学习"><span>上下文学习（情境学习）：</span></a></h4><figure><img src="https://raw.staticdn.net/Navyum/imgbed/pic/IMG/5053a72bfd83e3cd7408677d1ee5cbab.png" alt="Img" tabindex="0" loading="lazy"><figcaption>Img</figcaption></figure><ul><li>外循环：获得语言技能的无监督预训练</li><li>内循环：模型从示例序列中学习上下文，以预测接下来会发生什么</li><li>从GPT-2到GPT-3，随着参数规模的变化，情境学习才获得实质的突破</li></ul><h4 id="zero-shot" tabindex="-1"><a class="header-anchor" href="#zero-shot"><span>Zero-Shot：</span></a></h4><p>不允许演示，只提供自然语言的指导</p><h4 id="one-shot" tabindex="-1"><a class="header-anchor" href="#one-shot"><span>One-Shot：</span></a></h4><p>只允许一个演示</p><h4 id="few-shot" tabindex="-1"><a class="header-anchor" href="#few-shot"><span>Few-Shot：</span></a></h4><p>学习允许尽可能多的演示（通常为 10 到 100 个） <img src="https://raw.staticdn.net/Navyum/imgbed/pic/IMG/7f3ff204a9477f831a32fded2f10d10c.png" alt="Img" width="50%" loading="lazy"></p><p><img src="https://raw.staticdn.net/Navyum/imgbed/pic/IMG/962dd7a9f23f11285c539e784d5f4d98.png" alt="Img" loading="lazy"> “ppl”代表困惑，越低越好，“acc”代表准确度，越高越好 具有大容量的预训练语言模型可以通过自然语言指令和演示来利用其语言能力来理解和完成所需的任务</p><h2 id="参考" tabindex="-1"><a class="header-anchor" href="#参考"><span>参考：</span></a></h2><p><a href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf" target="_blank" rel="noopener noreferrer">Improving Language Understanding by Generative Pre-Training</a></p></div><!----><footer class="vp-page-meta"><!----><div class="vp-meta-item git-info"><div class="update-time"><span class="vp-meta-label">上次编辑于: </span><span class="vp-meta-info" data-allow-mismatch="text">2025/6/17 07:42:14</span></div><!----></div></footer><nav class="vp-page-nav"><a class="route-link auto-link prev" href="/%E5%A4%A7%E6%A8%A1%E5%9E%8BLLM/00.%E6%A6%82%E5%BF%B5/01.Transformer.html" aria-label="01.Transformer" iconsizing="both"><div class="hint"><span class="arrow start"></span>上一页</div><div class="link"><!---->01.Transformer</div></a><a class="route-link auto-link next" href="/%E5%A4%A7%E6%A8%A1%E5%9E%8BLLM/00.%E6%A6%82%E5%BF%B5/03.Prompt.html" aria-label="03.Prompt" iconsizing="both"><div class="hint">下一页<span class="arrow end"></span></div><div class="link">03.Prompt<!----></div></a></nav><div id="comment" class="waline-wrapper vp-comment" vp-comment darkmode="false" style="display:block;"><!----></div><!----><!--]--></main><!--]--><footer class="vp-footer-wrapper" vp-footer><!----><div class="vp-copyright">
  版权声明：自由转载 - 非商用 - 非衍生 - 保持署名<a href="https://creativecommons.org/licenses/by-nc-nd/4.0/deed.zh-hans" target="_blank" rel="noopener noreferrer">（创意共享 4.0 许可证）</a>|
  Copyright © 2024-present Navyum</a>
  </div></footer></div><!--]--><!--[--><!----><!--[--><!--]--><!--]--><!--]--></div>
    <script src="/assets/js/runtime~app.4343b326.js" defer></script><script src="/assets/js/1181.88d0b359.js" defer></script><script src="/assets/js/app.0aa5305f.js" defer></script>
    <!-- 看板娘区块 -->
    <script src="/live2d-widget/autoload.js"></script>
    <!-- End 看板娘区块 -->

    <!-- ahrefs分析-->
    <script src="https://analytics.ahrefs.com/analytics.js" data-key="7WraP4W+AhiZ1hvRy398kQ" async></script>

  </body>
</html>

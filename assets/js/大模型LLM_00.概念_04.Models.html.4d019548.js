"use strict";(self.webpackChunknavyum_blog=self.webpackChunknavyum_blog||[]).push([[16499],{11670:(a,e,t)=>{t.r(e),t.d(e,{comp:()=>r,data:()=>l});var i=t(6254);const n={},r=(0,t(36995).A)(n,[["render",function(a,e){return(0,i.uX)(),(0,i.CE)("div",null,e[0]||(e[0]=[(0,i.Fv)('<h3 id="vit-google-图像分类模型" tabindex="-1"><a class="header-anchor" href="#vit-google-图像分类模型"><span>ViT（Google 图像分类模型）</span></a></h3><ul><li>ViT是一种没有 CNN（卷积神经网络）的图像分类模型（Vision Transformer），从图像分类任务中消除了 CNN</li><li>ViT 将图像拆分为一系列图像补丁嵌入与位置编码混合，并将它们馈送到 Transformer 编码器中。ViT 有一个分类头 （MLP - 多层感知），它产生最终的预测 <img src="https://raw.staticdn.net/Navyum/imgbed/pic/IMG/3f10b60a7e8a4b98fece3f16f2e3a2f9.png" alt="Img" loading="lazy"></li><li>注意力可视化 <ul><li>ViT 可以捕捉图像中的主要对象，忽略图像中的噪声</li></ul></li><li>计算复杂度与输入图像大小呈<strong>二次方关系</strong></li></ul><h3 id="swin-transformer-microsoft-图像分类、对象检测、语义分割等视觉任务模型" tabindex="-1"><a class="header-anchor" href="#swin-transformer-microsoft-图像分类、对象检测、语义分割等视觉任务模型"><span>Swin Transformer（Microsoft 图像分类、对象检测、语义分割等视觉任务模型）</span></a></h3><ul><li>Shifted windows，它为 Transformer 提供了分层视野</li><li>Swin Transformer 从小尺寸的补丁开始，逐渐将相邻的补丁合并到更深的 Transformer 层中，从而构建分层特征图 <img src="https://raw.staticdn.net/Navyum/imgbed/pic/IMG/883ed585110507bf48ceeb0738167620.png" alt="Img" loading="lazy"></li><li>计算复杂度与输入图像大小呈<strong>线性关系</strong>。每个窗口中的补丁数量在任何级别都是相同的。最终，一个本地窗口用 16 个功能补丁覆盖整个映像。</li><li>Swin Transformer 的补丁合并通过连接 4 个 （= 2 x 2） 相邻补丁的特征并应用线性层来减少特征维度，从而减少补丁的数量</li><li>MSA 代表多头自我注意： <img src="https://raw.staticdn.net/Navyum/imgbed/pic/IMG/a286804e56372a6dfbfab218d6e72ee3.png" alt="Img" loading="lazy"> 使用图像色块向左上角的循环移动，然后应用遮罩机制来限制相邻色块内的自我注意。此外，它使用反向循环换档来覆盖其他区域</li></ul><h3 id="clip-openai-对比语言图像预训练模型" tabindex="-1"><a class="header-anchor" href="#clip-openai-对比语言图像预训练模型"><span>CLIP（OpenAI 对比语言图像预训练模型）</span></a></h3><ul><li>CLIP 即对比语言图像预训练（Contrastive Language Image Pre-training）。</li><li>通过对比学习，理解事物之间的差异，将特定的视觉特征与相应的描述性语言相关联，实现从文本生成图像。是从自然语言领域到视觉领域的桥梁，弥合语言和视觉之间差距方面的灵活性和潜力。</li><li>OpenAI 的 CLIP 模型同时训练图像编码器和文本编码器，以准确预测共享嵌入空间内（图像、文本）的正确配对，从而弥合视觉理解和自然语言理解之间的差距 <img src="https://raw.staticdn.net/Navyum/imgbed/pic/IMG/d1aae62fffba675c9dfd2a78a5fcbe0e.png" alt="Img" loading="lazy"></li></ul><h2 id="数据压缩和生成" tabindex="-1"><a class="header-anchor" href="#数据压缩和生成"><span>数据压缩和生成：</span></a></h2><h3 id="vae-变分自动编码器" tabindex="-1"><a class="header-anchor" href="#vae-变分自动编码器"><span>VAE（变分自动编码器）</span></a></h3><ul><li>VAE 通过识别和学习训练数据集中的隐藏特征并将其用作生成新数据，可以生成新的输出</li><li>有向概率模型，即贝叶斯网络。利用有向无环图 （DAG） 来说明各种随机变量之间的关系和依赖关系。 <img src="https://raw.staticdn.net/Navyum/imgbed/pic/IMG/c911644e66069245662d3ef9306df276.png" alt="Img" loading="lazy"><img src="https://raw.staticdn.net/Navyum/imgbed/pic/IMG/de3b3d6189e8e5f7560596c5170f317e.png" alt="Img" loading="lazy"></li><li>说明： 编码器负责对条件概率分布 P（z|x） 进行建模，因为它采用输入图像 x 并产生潜在表示 z。解码器负责对条件概率分布 P（x|z） 进行建模，因为它采用潜在变量 z 并重建靠近输入图像 x 的图像 x&#39;</li></ul><h2 id="长文本" tabindex="-1"><a class="header-anchor" href="#长文本"><span>长文本：</span></a></h2><h3 id="longformer" tabindex="-1"><a class="header-anchor" href="#longformer"><span>Longformer</span></a></h3><ul><li>引入新的注意力机制： <ul><li>背景：计算复杂度随序列长度呈二次方增加，这使得处理长序列的成本非常高。我们可以将一个长序列划分为更小的块，但这样我们将失去序列的全局上下文，从而降低语言任务的质量</li><li>Longformer 与序列长度呈线性关系，并且可以处理数千个令牌，这要归功于它的注意力模式。</li><li>Longfomer 不是完全的全局注意力，而是使用不同注意力模式的组合 <ul><li><p>Sliding Window Attention 推拉窗注意事项 <img src="https://raw.staticdn.net/Navyum/imgbed/pic/IMG/faf9abfce6195ac5af65fcf80e8b8c60.png" alt="Img" loading="lazy"></p></li><li><p>Dilated Sliding Window 扩张推拉窗 <img src="https://raw.staticdn.net/Navyum/imgbed/pic/IMG/178be1055d3e8738db9928eab11f80fa.png" alt="Img" loading="lazy"></p></li><li><p>Global + Sliding Window 全局+ 滑动窗口 <img src="https://raw.staticdn.net/Navyum/imgbed/pic/IMG/dec3a6bdcef0f15c7568955e11bcc45b.png" alt="Img" loading="lazy"><img src="https://raw.staticdn.net/Navyum/imgbed/pic/IMG/c4eda387c3aea582adba642da6a3900f.png" alt="Img" loading="lazy"></p></li></ul></li></ul></li></ul><h3 id="参考" tabindex="-1"><a class="header-anchor" href="#参考"><span>参考：</span></a></h3><p><a href="https://arxiv.org/pdf/2010.11929" target="_blank" rel="noopener noreferrer">ViT</a><a href="https://arxiv.org/pdf/2103.14030" target="_blank" rel="noopener noreferrer">Swin</a><a href="https://arxiv.org/pdf/1312.6114" target="_blank" rel="noopener noreferrer">VAE</a><a href="https://arxiv.org/pdf/2004.05150" target="_blank" rel="noopener noreferrer">Longformer</a></p>',14)]))}]]),l=JSON.parse('{"path":"/%E5%A4%A7%E6%A8%A1%E5%9E%8BLLM/00.%E6%A6%82%E5%BF%B5/04.Models.html","title":"04.Models","lang":"zh-CN","frontmatter":{"title":"04.Models","date":"2025-06-17T14:32:30.000Z","author":"Navyum","tags":["LLM","Prompt"],"categories":["LLM","AI"],"article":true,"index":true,"headerDepth":2,"sticky":false,"star":true,"description":"ViT（Google 图像分类模型） ViT是一种没有 CNN（卷积神经网络）的图像分类模型（Vision Transformer），从图像分类任务中消除了 CNN ViT 将图像拆分为一系列图像补丁嵌入与位置编码混合，并将它们馈送到 Transformer 编码器中。ViT 有一个分类头 （MLP - 多层感知），它产生最终的预测 Img 注意力可视...","head":[["meta",{"property":"og:url","content":"https://myblog.camscanner.top/%E5%A4%A7%E6%A8%A1%E5%9E%8BLLM/00.%E6%A6%82%E5%BF%B5/04.Models.html"}],["meta",{"property":"og:site_name","content":"Navyum\'s Blog"}],["meta",{"property":"og:title","content":"04.Models"}],["meta",{"property":"og:description","content":"ViT（Google 图像分类模型） ViT是一种没有 CNN（卷积神经网络）的图像分类模型（Vision Transformer），从图像分类任务中消除了 CNN ViT 将图像拆分为一系列图像补丁嵌入与位置编码混合，并将它们馈送到 Transformer 编码器中。ViT 有一个分类头 （MLP - 多层感知），它产生最终的预测 Img 注意力可视..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://raw.staticdn.net/Navyum/imgbed/pic/IMG/3f10b60a7e8a4b98fece3f16f2e3a2f9.png"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-06-17T07:42:14.000Z"}],["meta",{"property":"article:author","content":"Navyum"}],["meta",{"property":"article:tag","content":"LLM"}],["meta",{"property":"article:tag","content":"Prompt"}],["meta",{"property":"article:published_time","content":"2025-06-17T14:32:30.000Z"}],["meta",{"property":"article:modified_time","content":"2025-06-17T07:42:14.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"04.Models\\",\\"image\\":[\\"https://raw.staticdn.net/Navyum/imgbed/pic/IMG/3f10b60a7e8a4b98fece3f16f2e3a2f9.png\\",\\"https://raw.staticdn.net/Navyum/imgbed/pic/IMG/883ed585110507bf48ceeb0738167620.png\\",\\"https://raw.staticdn.net/Navyum/imgbed/pic/IMG/a286804e56372a6dfbfab218d6e72ee3.png\\",\\"https://raw.staticdn.net/Navyum/imgbed/pic/IMG/d1aae62fffba675c9dfd2a78a5fcbe0e.png\\",\\"https://raw.staticdn.net/Navyum/imgbed/pic/IMG/c911644e66069245662d3ef9306df276.png\\",\\"https://raw.staticdn.net/Navyum/imgbed/pic/IMG/de3b3d6189e8e5f7560596c5170f317e.png\\",\\"https://raw.staticdn.net/Navyum/imgbed/pic/IMG/faf9abfce6195ac5af65fcf80e8b8c60.png\\",\\"https://raw.staticdn.net/Navyum/imgbed/pic/IMG/178be1055d3e8738db9928eab11f80fa.png\\",\\"https://raw.staticdn.net/Navyum/imgbed/pic/IMG/dec3a6bdcef0f15c7568955e11bcc45b.png\\",\\"https://raw.staticdn.net/Navyum/imgbed/pic/IMG/c4eda387c3aea582adba642da6a3900f.png\\"],\\"datePublished\\":\\"2025-06-17T14:32:30.000Z\\",\\"dateModified\\":\\"2025-06-17T07:42:14.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Navyum\\"}]}"]]},"git":{"createdTime":1749983452000,"updatedTime":1750146134000,"contributors":[{"name":"Navyum","username":"Navyum","email":"36869790+Navyum@users.noreply.github.com","commits":1,"url":"https://github.com/Navyum"},{"name":"haijun_yang","username":"haijun_yang","email":"haijun_yang@intsig.net","commits":1,"url":"https://github.com/haijun_yang"}]},"readingTime":{"minutes":3.23,"words":968},"filePathRelative":"大模型LLM/00.概念/04.Models.md","localizedDate":"2025年6月17日","excerpt":"<h3>ViT（Google 图像分类模型）</h3>\\n<ul>\\n<li>ViT是一种没有 CNN（卷积神经网络）的图像分类模型（Vision Transformer），从图像分类任务中消除了 CNN</li>\\n<li>ViT 将图像拆分为一系列图像补丁嵌入与位置编码混合，并将它们馈送到 Transformer 编码器中。ViT 有一个分类头 （MLP - 多层感知），它产生最终的预测\\n<img src=\\"https://raw.staticdn.net/Navyum/imgbed/pic/IMG/3f10b60a7e8a4b98fece3f16f2e3a2f9.png\\" alt=\\"Img\\" loading=\\"lazy\\"></li>\\n<li>注意力可视化\\n<ul>\\n<li>ViT 可以捕捉图像中的主要对象，忽略图像中的噪声</li>\\n</ul>\\n</li>\\n<li>计算复杂度与输入图像大小呈<strong>二次方关系</strong></li>\\n</ul>","autoDesc":true}')},36995:(a,e)=>{e.A=(a,e)=>{const t=a.__vccOpts||a;for(const[a,i]of e)t[a]=i;return t}}}]);